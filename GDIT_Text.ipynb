{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Topic/Document Classification\n",
    "Using GDI text dataset, we set to develop a deep learning model to classify text with high performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert-embedding\r\n",
      "  Downloading https://files.pythonhosted.org/packages/62/85/e0d56e29a055d8b3ba6da6e52afe404f209453057de95b90c01475c3ff75/bert_embedding-1.0.1-py3-none-any.whl\r\n",
      "Collecting mxnet==1.4.0 (from bert-embedding)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/e9/241aadccc4522f99adee5b6043f730d58adb7c001e0a68865a3728c3b4ae/mxnet-1.4.0-py2.py3-none-manylinux1_x86_64.whl (29.6MB)\r\n",
      "\u001b[K     |████████████████████████████████| 29.6MB 4.8MB/s \r\n",
      "\u001b[?25hCollecting numpy==1.14.6 (from bert-embedding)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/c4/395ebb218053ba44d64935b3729bc88241ec279915e72100c5979db10945/numpy-1.14.6-cp36-cp36m-manylinux1_x86_64.whl (13.8MB)\r\n",
      "\u001b[K     |████████████████████████████████| 13.8MB 19.7MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: typing==3.6.6 in /opt/conda/lib/python3.6/site-packages (from bert-embedding) (3.6.6)\r\n",
      "Collecting gluonnlp==0.6.0 (from bert-embedding)\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e2/07/037585c23bccec19ce333b402997d98b09e43cc8d2d86dc810d57249c5ff/gluonnlp-0.6.0.tar.gz (209kB)\r\n",
      "\u001b[K     |████████████████████████████████| 215kB 33.4MB/s \r\n",
      "\u001b[?25hRequirement already satisfied: graphviz<0.9.0,>=0.8.1 in /opt/conda/lib/python3.6/site-packages (from mxnet==1.4.0->bert-embedding) (0.8.4)\r\n",
      "Requirement already satisfied: requests>=2.20.0 in /opt/conda/lib/python3.6/site-packages (from mxnet==1.4.0->bert-embedding) (2.22.0)\r\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.20.0->mxnet==1.4.0->bert-embedding) (2.8)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.20.0->mxnet==1.4.0->bert-embedding) (2019.9.11)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests>=2.20.0->mxnet==1.4.0->bert-embedding) (1.24.2)\r\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests>=2.20.0->mxnet==1.4.0->bert-embedding) (3.0.4)\r\n",
      "Building wheels for collected packages: gluonnlp\r\n",
      "  Building wheel for gluonnlp (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for gluonnlp: filename=gluonnlp-0.6.0-cp36-none-any.whl size=259917 sha256=7444bad05fb098d68242eb0604019b3015571d6f21c3275363f3ea43bfe64a9f\r\n",
      "  Stored in directory: /tmp/.cache/pip/wheels/ff/48/ac/a77c79aa416ba6dd7bf487f2280b0471034f66141617965914\r\n",
      "Successfully built gluonnlp\r\n",
      "\u001b[31mERROR: allennlp 0.9.0 requires flaky, which is not installed.\u001b[0m\r\n",
      "\u001b[31mERROR: allennlp 0.9.0 requires responses>=0.7, which is not installed.\u001b[0m\r\n",
      "\u001b[31mERROR: tsfresh 0.12.0 has requirement pandas<=0.23.4,>=0.20.3, but you'll have pandas 0.25.1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mERROR: spacy 2.1.8 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mERROR: ray 0.7.5 has requirement protobuf>=3.8.0, but you'll have protobuf 3.7.1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mERROR: phik 0.9.8 has requirement numpy>=1.15.4, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mERROR: osmnx 0.10 has requirement numpy>=1.16, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mERROR: mxnet-cu100 1.5.0 has requirement numpy<2.0.0,>1.16.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mERROR: mlxtend 0.17.0 has requirement numpy>=1.16.2, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mERROR: mizani 0.6.0 has requirement matplotlib>=3.1.1, but you'll have matplotlib 3.0.3 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mERROR: librosa 0.7.0 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mERROR: kmeans-smote 0.1.2 has requirement imbalanced-learn<0.5,>=0.4.0, but you'll have imbalanced-learn 0.5.0 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mERROR: kmeans-smote 0.1.2 has requirement scikit-learn<0.21,>=0.19.0, but you'll have scikit-learn 0.21.3 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mERROR: hyperopt 0.1.2 has requirement networkx==2.2, but you'll have networkx 2.3 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mERROR: fastai 1.0.57 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mERROR: cvxpy 1.0.25 has requirement numpy>=1.15, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mERROR: cufflinks 0.16 has requirement plotly<4.0.0a0,>=3.0.0, but you'll have plotly 4.1.1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mERROR: catboost 0.17.3 has requirement numpy>=1.16.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mERROR: blis 0.2.4 has requirement numpy>=1.15.0, but you'll have numpy 1.14.6 which is incompatible.\u001b[0m\r\n",
      "Installing collected packages: numpy, mxnet, gluonnlp, bert-embedding\r\n",
      "  Found existing installation: numpy 1.16.4\r\n",
      "    Uninstalling numpy-1.16.4:\r\n",
      "      Successfully uninstalled numpy-1.16.4\r\n",
      "  Found existing installation: gluonnlp 0.8.1\r\n",
      "    Uninstalling gluonnlp-0.8.1:\r\n",
      "      Successfully uninstalled gluonnlp-0.8.1\r\n",
      "Successfully installed bert-embedding-1.0.1 gluonnlp-0.6.0 mxnet-1.4.0 numpy-1.14.6\r\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-embedding\n",
    "from bert_embedding import BertEmbedding\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from fastai.vision import Path\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import cosine_similarity as cs\n",
    "import operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to load up te text files and its topics into a pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:03<00:00,  1.30s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>business</td>\n",
       "      <td>FAO warns on impact of subsidies\\n\\nBillions o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "      <td>Singapore growth at 8.1% in 2004\\n\\nSingapore'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>business</td>\n",
       "      <td>Bank payout to Pinochet victims\\n\\nA US bank h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Target                                               Text\n",
       "0  business  FAO warns on impact of subsidies\\n\\nBillions o...\n",
       "1  business  Singapore growth at 8.1% in 2004\\n\\nSingapore'...\n",
       "2  business  Bank payout to Pinochet victims\\n\\nA US bank h..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = [\"business\", \"politics\", \"sport\"]\n",
    "targets = []\n",
    "texts = []\n",
    "for c in tqdm(classes):\n",
    "    for tf in Path(f\"../input/PatriotHack-master/data/{c}/{c}\").ls():\n",
    "        with open(tf, 'r', encoding = 'unicode_escape') as text:\n",
    "            targets.append(c)\n",
    "            texts.append(text.read())\n",
    "df = pd.DataFrame()\n",
    "df[\"Target\"] = targets\n",
    "df[\"Text\"] = texts\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we load up a pretrained Bert model for word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab file is not found. Downloading.\n",
      "Downloading /tmp/.mxnet/models/book_corpus_wiki_en_uncased-a6607397.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/vocab/book_corpus_wiki_en_uncased-a6607397.zip...\n",
      "Downloading /tmp/.mxnet/models/bert_12_768_12_book_corpus_wiki_en_uncased-75cc780f.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/models/bert_12_768_12_book_corpus_wiki_en_uncased-75cc780f.zip...\n"
     ]
    }
   ],
   "source": [
    "be = BertEmbedding(model='bert_12_768_12', dataset_name='book_corpus_wiki_en_uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to then create embeddings for each document. We do this by taking the mean of all the word embeddings in the document. We also noticed there are empty text files so we drop that entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  `ndarray`, however any non-default value will be.  If the\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Text</th>\n",
       "      <th>Embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>business</td>\n",
       "      <td>FAO warns on impact of subsidies\\n\\nBillions o...</td>\n",
       "      <td>[[-0.21912901, 0.19704409, 0.203581, 0.3530849...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "      <td>Singapore growth at 8.1% in 2004\\n\\nSingapore'...</td>\n",
       "      <td>[[-0.7985942, -0.19978853, 0.35024175, 0.33936...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>business</td>\n",
       "      <td>Bank payout to Pinochet victims\\n\\nA US bank h...</td>\n",
       "      <td>[[-0.02139423, -0.47071832, 0.16319667, 0.1360...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Target                                               Text  \\\n",
       "0  business  FAO warns on impact of subsidies\\n\\nBillions o...   \n",
       "1  business  Singapore growth at 8.1% in 2004\\n\\nSingapore'...   \n",
       "2  business  Bank payout to Pinochet victims\\n\\nA US bank h...   \n",
       "\n",
       "                                           Embedding  \n",
       "0  [[-0.21912901, 0.19704409, 0.203581, 0.3530849...  \n",
       "1  [[-0.7985942, -0.19978853, 0.35024175, 0.33936...  \n",
       "2  [[-0.02139423, -0.47071832, 0.16319667, 0.1360...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getVector(sent): return np.mean(np.array(be([sent], 'sum')[0][1]), axis=0)[None]\n",
    "df[\"Embedding\"] = df[\"Text\"].apply(lambda x: getVector(x))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,i in enumerate(df[\"Embedding\"]):\n",
    "    try: i.shape[1]\n",
    "    except: df = df.drop(x, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we get the embeddings for all of the topics by taking the mean of document embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'business': array([[-0.100995, -0.120632,  0.27129 ,  0.031712, ..., -0.031566, -0.290568,  0.156345, -0.109788]], dtype=float32),\n",
       " 'politics': array([[-0.058113, -0.165831,  0.244364, -0.262047, ..., -0.001339, -0.29378 , -0.144061,  0.133773]], dtype=float32),\n",
       " 'sport': array([[-0.175366, -0.068401,  0.310909, -0.25563 , ..., -0.332931, -0.143316, -0.109809, -0.05016 ]], dtype=float32)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cembs = {}\n",
    "for c in classes: cembs[c] = np.mean(np.array(df[df[\"Target\"] == c][\"Embedding\"]), axis=0)\n",
    "cembs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets test it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a business article about the stock market. We find that the price of Google will skyrocket.\"\n",
    "query = \" \".join(text.split(\" \"))\n",
    "qemb = getVector(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we check what topic this document is in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for k,v in cembs.items():\n",
    "    results[k] =  cs(v, qemb)[0][0]\n",
    "results = sorted(results.items(), key=operator.itemgetter(1))\n",
    "subsection = df[df[\"Target\"]==results[-1][0]]\n",
    "print(results[-1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we find the most relevant document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dollar hits new low versus euro\\n\\nThe US dollar has continued its record-breaking slide and has tumbled to a new low against the euro.\\n\\nInvestors are betting that the European Central Bank (ECB) will not do anything to weaken the euro, while the US is thought to favour a declining dollar. The US is struggling with a ballooning trade deficit and analysts said one of the easiest ways to fund it was by allowing a depreciation of the dollar. They have predicted that the dollar is likely to fall even further.\\n\\nThe US currency was trading at $1.364 per euro at 1800 GMT on Monday. This compares with $1.354 to the euro in late trading in New York on Friday, which was then a record low.\\n\\nThe dollar has weakened sharply since September when it traded about $1.20 against the euro. It has lost 7% this year, while against the Japanese yen it is down 3.2%. Traders said that thin trading levels had amplified Monday\\'s move. \"It\\'s not going to take much to push [the dollar] one way or the other,\" said Grant Wilson of Mellon Bank. Liquidity - a measure of the number of parties willing to trade in the market - was about half that of a normal working day, traders said.\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eresults = {}\n",
    "for x, doc in zip(subsection[\"Text\"],subsection[\"Embedding\"]):\n",
    "    eresults[x] =  cs(doc, qemb)[0][0]\n",
    "sorted(eresults.items(), key=operator.itemgetter(1))[-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a business article about the stock market. We find that the price of Google will skyrocket.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "# df[\"Text\"] = df[\"Text\"].apply(lambda x: \"[CLS] \"+re.sub(r\"[\\n.,-]\", \" [SEP] \", x).replace(\"\\\"\", \"\").replace(\"  \", \" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
